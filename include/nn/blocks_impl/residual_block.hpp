/*
 * Copyright (c) 2025 Tung D. Pham
 *
 * This software is licensed under the MIT License. See the LICENSE file in the
 * project root for the full license text.
 */
#pragma once

#include "nn/activations.hpp"
#include "nn/layers_impl/base_layer.hpp"
#include "ops/ops.hpp"

#include <memory>
#include <string>
#include <unordered_map>
#include <utility>
#include <vector>

namespace tnn {

/**
 * @brief Residual block implementing skip connections: output = F(x) + x
 *
 * Supports both identity shortcuts (when input/output dimensions match)
 * and projection shortcuts (1x1 conv when dimensions differ).
 */
template <typename T = float> class ResidualBlock : public Layer<T> {
private:
  std::vector<std::unique_ptr<Layer<T>>> main_path_;
  std::unique_ptr<Layer<T>> shortcut_;
  std::unique_ptr<ActivationFunction<T>> final_activation_;

  std::unordered_map<size_t, Tensor<T>> main_output_cache_;
  std::unordered_map<size_t, Tensor<T>> shortcut_output_cache_;
  std::unordered_map<size_t, Tensor<T>> pre_activation_cache_;

  std::string activation_type_;

public:
  /**
   * @brief Constructs a residual block
   * @param main_path The main transformation path F(x) as a vector of layers
   * @param shortcut Optional projection layer for dimension matching (nullptr for identity)
   * @param final_activation Activation applied after addition (e.g., "relu")
   * @param name Layer name
   */
  ResidualBlock(std::vector<std::unique_ptr<Layer<T>>> main_path,
                std::unique_ptr<Layer<T>> shortcut = nullptr,
                const std::string &final_activation = "relu",
                const std::string &name = "residual_block")
      : main_path_(std::move(main_path)), shortcut_(std::move(shortcut)),
        activation_type_(final_activation) {
    this->name_ = name;

    if (final_activation != "none" && final_activation != "linear") {
      auto factory = ActivationFactory<T>();
      factory.register_defaults();
      final_activation_ = factory.create(final_activation);
    }
  }

  ResidualBlock(const ResidualBlock &other) : activation_type_(other.activation_type_) {
    this->name_ = other.name_;
    this->is_training_ = other.is_training_;

    for (const auto &layer : other.main_path_) {
      main_path_.push_back(layer->clone());
    }

    if (other.shortcut_) {
      shortcut_ = other.shortcut_->clone();
    }

    if (other.final_activation_) {
      auto factory = ActivationFactory<T>();
      factory.register_defaults();
      final_activation_ = factory.create(activation_type_);
    }
  }

  void initialize() override {
    for (auto &layer : main_path_) {
      layer->initialize();
    }
    if (shortcut_) {
      shortcut_->initialize();
    }
  }

  const Tensor<T> &forward(const Tensor<T> &input, size_t micro_batch_id = 0) override {
    const Tensor<T> &current_input =
        input.device() == this->device_ ? input : input.to_device(this->get_device());

    // Main path: F(x)
    const Tensor<T> *current = &current_input;
    for (auto &layer : main_path_) {
      current = &layer->forward(*current, micro_batch_id);
    }
    auto it_main = main_output_cache_.find(micro_batch_id);
    if (it_main == main_output_cache_.end()) {
      main_output_cache_[micro_batch_id] = current->clone();
    } else {
      it_main->second.resize(current->shape());
      ops::copy(current->data_ptr(), it_main->second.data_ptr(), current->size());
    }

    // Shortcut path: x or projection(x)
    const Tensor<T> &shortcut_output =
        shortcut_ ? shortcut_->forward(current_input, micro_batch_id) : current_input;
    auto it_shortcut = shortcut_output_cache_.find(micro_batch_id);
    if (it_shortcut == shortcut_output_cache_.end()) {
      shortcut_output_cache_[micro_batch_id] = shortcut_output.clone();
    } else {
      shortcut_output_cache_[micro_batch_id].resize(current_input.shape());
      if (shortcut_) {
        ops::copy(shortcut_output.data_ptr(), it_shortcut->second.data_ptr(),
                  shortcut_output.size());
      } else {
        ops::copy(current_input.data_ptr(), it_shortcut->second.data_ptr(), current_input.size());
      }
    }

    // Residual connection: F(x) + x
    Tensor<T> &output =
        this->get_output_buffer(micro_batch_id, main_output_cache_[micro_batch_id].shape());
    ops::add(main_output_cache_[micro_batch_id].data_ptr(),
             shortcut_output_cache_[micro_batch_id].data_ptr(), output.data_ptr(), output.size());

    // Cache pre-activation output for backward pass
    auto it_pre_act = pre_activation_cache_.find(micro_batch_id);
    if (it_pre_act == pre_activation_cache_.end()) {
      pre_activation_cache_[micro_batch_id] = output.clone();
    } else {
      it_pre_act->second.resize(output.shape());
      ops::copy(it_pre_act->second.data_ptr(), output.data_ptr(), output.size());
    }

    if (final_activation_) {
      final_activation_->apply(output);
    }

    return output;
  }

  const Tensor<T> &backward(const Tensor<T> &gradient, size_t micro_batch_id = 0) override {
    const Tensor<T> &current_gradient =
        gradient.device() == this->device_ ? gradient : gradient.to_device(this->get_device());

    // Gradient through final activation
    Tensor<T> grad_after_activation;
    if (final_activation_) {
      grad_after_activation = current_gradient.clone();
      final_activation_->compute_gradient_inplace(pre_activation_cache_[micro_batch_id],
                                                  grad_after_activation);
    }
    const Tensor<T> &grad_to_propagate =
        final_activation_ ? grad_after_activation : current_gradient;

    // Backward through main path
    const Tensor<T> *grad_main = &grad_to_propagate;
    for (int i = static_cast<int>(main_path_.size()) - 1; i >= 0; --i) {
      grad_main = &main_path_[i]->backward(*grad_main, micro_batch_id);
    }

    // Backward through shortcut
    const Tensor<T> *grad_shortcut;
    if (shortcut_) {
      grad_shortcut = &shortcut_->backward(grad_to_propagate, micro_batch_id);
    } else {
      grad_shortcut = &grad_to_propagate;
    }

    // Sum gradients from both paths
    Tensor<T> &grad_input = this->get_gradient_buffer(micro_batch_id, grad_main->shape());
    ops::add(grad_main->data_ptr(), grad_shortcut->data_ptr(), grad_input.data_ptr(),
             grad_input.size());
    return grad_input;
  }

  std::vector<Tensor<T> *> parameters() override {
    std::vector<Tensor<T> *> params;
    for (auto &layer : main_path_) {
      auto layer_params = layer->parameters();
      params.insert(params.end(), layer_params.begin(), layer_params.end());
    }

    if (shortcut_) {
      auto shortcut_params = shortcut_->parameters();
      params.insert(params.end(), shortcut_params.begin(), shortcut_params.end());
    }

    return params;
  }

  std::vector<Tensor<T> *> gradients() override {
    std::vector<Tensor<T> *> grads;
    for (auto &layer : main_path_) {
      auto layer_grads = layer->gradients();
      grads.insert(grads.end(), layer_grads.begin(), layer_grads.end());
    }

    if (shortcut_) {
      auto shortcut_grads = shortcut_->gradients();
      grads.insert(grads.end(), shortcut_grads.begin(), shortcut_grads.end());
    }

    return grads;
  }

  bool has_parameters() const override {
    for (const auto &layer : main_path_) {
      if (layer->has_parameters())
        return true;
    }
    return (shortcut_ && shortcut_->has_parameters());
  }

  void set_training(bool training) override {
    this->is_training_ = training;
    for (auto &layer : main_path_) {
      layer->set_training(training);
    }
    if (shortcut_) {
      shortcut_->set_training(training);
    }
  }

  void set_device(const Device *device) override {
    Layer<T>::set_device(device);
    for (auto &layer : main_path_) {
      layer->set_device(device);
    }
    if (shortcut_) {
      shortcut_->set_device(device);
    }
  }

  std::vector<size_t> compute_output_shape(const std::vector<size_t> &input_shape) const override {
    std::vector<size_t> shape = input_shape;
    for (const auto &layer : main_path_) {
      shape = layer->compute_output_shape(shape);
    }
    return shape;
  }

  uint64_t forward_complexity(const std::vector<size_t> &input_shape) const override {
    uint64_t main_complexity = 0;
    std::vector<size_t> current_shape = input_shape;
    for (const auto &layer : main_path_) {
      main_complexity += layer->forward_complexity(current_shape);
      current_shape = layer->compute_output_shape(current_shape);
    }

    uint64_t shortcut_complexity = 0;
    if (shortcut_) {
      shortcut_complexity = shortcut_->forward_complexity(input_shape);
    }

    // Add complexity for element-wise addition (approximately input_size operations)
    size_t add_complexity = 1;
    for (size_t dim : input_shape) {
      add_complexity *= dim;
    }

    return main_complexity + shortcut_complexity + add_complexity;
  }

  uint64_t backward_complexity(const std::vector<size_t> &input_shape) const override {
    uint64_t main_complexity = 0;
    std::vector<size_t> current_shape = input_shape;
    for (const auto &layer : main_path_) {
      main_complexity += layer->backward_complexity(current_shape);
      current_shape = layer->compute_output_shape(current_shape);
    }

    uint64_t shortcut_complexity = 0;
    if (shortcut_) {
      shortcut_complexity = shortcut_->backward_complexity(input_shape);
    }

    size_t add_complexity = 1;
    for (size_t dim : input_shape) {
      add_complexity *= dim;
    }

    return main_complexity + shortcut_complexity + add_complexity;
  }

  uint64_t forward_flops(const std::vector<size_t> &input_shape) const override {
    return forward_complexity(input_shape);
  }

  uint64_t backward_flops(const std::vector<size_t> &input_shape) const override {
    return backward_complexity(input_shape);
  }

  std::string type() const override { return "ResidualBlock"; }

  LayerConfig get_config() const override {
    LayerConfig config;
    config.name = this->name_;
    config.parameters["activation"] = activation_type_;
    config.parameters["has_projection"] = (shortcut_ != nullptr);
    return config;
  }

  std::unique_ptr<Layer<T>> clone() const override {
    std::vector<std::unique_ptr<Layer<T>>> main_clone;
    for (const auto &layer : main_path_) {
      main_clone.push_back(layer->clone());
    }
    auto shortcut_clone = shortcut_ ? shortcut_->clone() : nullptr;
    return std::make_unique<ResidualBlock<T>>(std::move(main_clone), std::move(shortcut_clone),
                                              activation_type_, this->name_);
  }

  const std::vector<std::unique_ptr<Layer<T>>> &get_main_path() const { return main_path_; }
  const Layer<T> *get_shortcut() const { return shortcut_.get(); }
};
} // namespace tnn
