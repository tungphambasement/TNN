# Training configuration
MODEL_NAME=cifar10_resnet9
# MODEL_PATH=model_snapshots/cifar10_resnet9
DATASET_NAME=cifar10
DATASET_PATH=data/cifar-10-batches-bin

# MODEL_NAME=flash_gpt2
# # MODEL_PATH=model_snapshots/gpt2
# DATASET_NAME=open_webtext
# DATASET_PATH=data/open-web-text/

# MODEL_NAME=tiny_imagenet_resnet50
# # MODEL_PATH=model_snapshots/tiny_imagenet_vit
# DATASET_NAME=tiny_imagenet
# DATASET_PATH=data/tiny-imagenet-200/

# MODEL_NAME=mnist_cnn
# # MODEL_PATH=model_snapshots/mnist_cnn
# DATASET_NAME=mnist
# DATASET_PATH=data/mnist

# Coordinator configuration
COORDINATOR_HOST=localhost
COORDINATOR_PORT=9000

# Worker hosts configuration
WORKER1_HOST=localhost
WORKER1_PORT=8001

WORKER2_HOST=localhost
WORKER2_PORT=8002
# more workers can be added similarly, note to add it to the coordinator config as well

# Device configuration
DEVICE_TYPE=GPU

# Training parameters
BATCH_SIZE=128
EPOCHS=10
MAX_STEPS=-1
LR_INITIAL=0.0006
GRADIENT_ACCUMULATION_STEPS=1

NUM_MICROBATCHES=1

# Logging
LOG_LEVEL=INFO
PROGRESS_PRINT_INTERVAL=10
PROFILER_TYPE=NONE
PRINT_LAYER_PROFILING=0
PRINT_LAYER_MEMORY_USAGE=0